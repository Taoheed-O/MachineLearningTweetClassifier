{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cInmTEWmpZTZ"
   },
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "### Importing the libraries needed for building the model.\n",
    "  - numpy to do some numerical operations\n",
    "  - pandas for cleaning and other processings needed by the data\n",
    "  - matplotlib for visualizing if there's a need to\n",
    "  - re[regular expressions for doing some cleaning on the text data itself:\n",
    "    - to remove some prefix and suffixes\n",
    "    - to remove emoji like comments\n",
    "    - to remove @ keyword etc\n",
    "  - bs4 to scrape the the website for the slangs used by Nigerians\n",
    "  - requests needed for the url of websites needed for scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nmqm5CySJ3bI"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8CmJk-LqZ7X"
   },
   "source": [
    "# Scraping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "768o4VvUKFd2"
   },
   "outputs": [],
   "source": [
    "url = \"https://insight.ng/spice/nigerian-slangs-dictionary/\"\n",
    "req = requests.get(url)\n",
    "soup = bs(req.text, \"html.parser\")\n",
    "slangs = soup.find(\"ul\", attrs = {\"class\": \"ez-toc-list-level-3\"} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXbUmwXyqemJ"
   },
   "source": [
    "# Creating a list for slang from the first website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-oKNUIJKTA2",
    "outputId": "ad64cd34-cdc9-4174-b34c-7ca4a5613e48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_list = []\n",
    "for slang in slangs.select('a'):\n",
    "  slang_list.append(slang.text)\n",
    "# print(slang_list)\n",
    "len(slang_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xokOZ5AnoKor"
   },
   "outputs": [],
   "source": [
    "new_slang_list = []\n",
    "for word in slang_list:\n",
    "  word = word.split('/')\n",
    "  new_slang_list.extend(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EFhdx2xbqUq3"
   },
   "outputs": [],
   "source": [
    "cleaned_slang_list_1 = []\n",
    "for word in new_slang_list:\n",
    "  word = word.replace('\\xa0', '')\n",
    "  word =  word.replace('.','')\n",
    "  word = word.replace(':','')\n",
    "  word = re.sub(\"\\d+\", '',word)\n",
    "  word = word.strip()\n",
    "  cleaned_slang_list_1.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPJh9BloqoNV"
   },
   "source": [
    "# Cleaning the texts gotten from the website for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "slGeD9-nrPjx"
   },
   "outputs": [],
   "source": [
    "unknown = ['About Author','Latest entries','+']\n",
    "for word in cleaned_slang_list_1:\n",
    "  if word in unknown:\n",
    "    cleaned_slang_list_1.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xifScz4oqxOh"
   },
   "source": [
    "### 58 Texts gotten from the first website after scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaIoVNwfwUpk",
    "outputId": "e423ed12-a881-4756-a35e-3e348eace67b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_slang_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6WKNhuB8II_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKM8660Tq3d8"
   },
   "source": [
    "# Scraping process for the second website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sz3IzxPUC_bC"
   },
   "outputs": [],
   "source": [
    "slangs_2_list = []\n",
    "\n",
    "url_2 = \"https://www.skabash.com/popular-nigerian-slangs-and-their-meanings/\"\n",
    "req_2 = requests.get(url_2)\n",
    "soup_2 = bs(req_2.text, \"html.parser\")\n",
    "slangs_2 = soup_2.find(\"div\", attrs={\"class\":\"lwptoc_items lwptoc_items-visible\"})\n",
    "\n",
    "for element in slangs_2.find(\"div\").select(\"div\"):\n",
    "  slangs_2_list.append(element.find('a').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6Rt1ma44DF9C"
   },
   "outputs": [],
   "source": [
    "new_slang_list_2 = []\n",
    "\n",
    "for slang in slangs_2_list:\n",
    "  slang = slang.replace('\\n', '')\n",
    "  slang = re.sub('\\d','',slang)\n",
    "  slang = slang.replace('.', '')\n",
    "  new_slang_list_2.append(slang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P99f4dY5DIXJ"
   },
   "outputs": [],
   "source": [
    "new_slang_list_2.remove(\"Popular Nigerian slangs that are trending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "y8pf_heULLCE"
   },
   "outputs": [],
   "source": [
    "new_slang_list_2 = list(set(new_slang_list_2))\n",
    "\n",
    "cleaned_slang_list_2 = []\n",
    "for word in new_slang_list_2:\n",
    "  word = word.split('/')\n",
    "  cleaned_slang_list_2.extend(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EiPzi0qq835"
   },
   "source": [
    "### 33 slangs gotten from the second website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stG106VDMRuC",
    "outputId": "f4f6b8c9-e308-4844-856d-7636e2ab0db8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_slang_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG-fXv0drCFj"
   },
   "source": [
    "### Combining the two slang lists into a single list by mere list addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbiAxgCEMz7Z",
    "outputId": "7e50ae1a-7ab7-48a2-969a-4b451729c213"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_list = cleaned_slang_list_1 + cleaned_slang_list_2\n",
    "len(long_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_list = [\"really very good\", \"very great and nice\", \"pretty nice\",\n",
    "              \"God bless\", \"pretty nice\", \"better things\", \"Lord is good\", \"Bright and beautiful\", \"going well\",\n",
    "             \"soft life\", \"good news\", \"greater things\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRePpJ4ErIAo"
   },
   "source": [
    "# SCRAPING THE **TWITTER** WEB IN SEARCH OF THE TWEETS THAT CONTAINS THE ABOVE WORDS USING __SNSCRAPE__ LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwC5FgidQji_",
    "outputId": "69ffa23c-7910-454b-ce7a-f786574a1353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (0.6.2.20230320)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from snscrape) (2.28.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from snscrape) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from snscrape) (4.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from snscrape) (3.6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BdUiLgOKM08o"
   },
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as snstwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEcrGAlGrZZ6"
   },
   "source": [
    "# THE SCRAPING PROCESS\n",
    "\n",
    "  - The list was gotten by specifying the country's location via the coordinates as shown with the variable loc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lA7AedXfPRvY",
    "outputId": "183c448b-f946-4e16-c444-d64405cbdde4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oyeni\\AppData\\Local\\Temp\\ipykernel_18504\\2310741059.py:7: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tweet_list.append([item.content, item.likeCount, item.user.location])\n"
     ]
    }
   ],
   "source": [
    "loc = '9.077751, 8.6774567, 100km'\n",
    "tweet_list = []\n",
    "for word in long_list:\n",
    "  for i, item in enumerate(snstwitter.TwitterSearchScraper('{} geocode:\"{}\"'.format(word, loc)).get_items()):\n",
    "    if i > 150:\n",
    "      break\n",
    "    tweet_list.append([item.content, item.likeCount, item.user.location])\n",
    "df = pd.DataFrame(tweet_list, columns = [\"tweets\", \"likes\", \"location\"])\n",
    "df.to_csv(\"slang_tweets_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oyeni\\AppData\\Local\\Temp\\ipykernel_18504\\4064895595.py:7: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tweet_list.append([item.content, item.likeCount, item.user.location])\n"
     ]
    }
   ],
   "source": [
    "loc = '9.077751, 8.6774567, 100km'\n",
    "tweet_list = []\n",
    "for word in short_list:\n",
    "  for i, item in enumerate(snstwitter.TwitterSearchScraper('{} geocode:\"{}\"'.format(word, loc)).get_items()):\n",
    "    if i > 150:\n",
    "      break\n",
    "    tweet_list.append([item.content, item.likeCount, item.user.location])\n",
    "df_short = pd.DataFrame(tweet_list, columns = [\"tweets\", \"likes\", \"location\"])\n",
    "df_short.to_csv(\"slang_tweets_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zh9RHE6qXWQQ",
    "outputId": "3eafc259-bc26-43c7-9c4f-ff13279c06b2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@AITraderGlobal Very brilliant &amp;amp; interesti...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abuja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Men really love the mean girls. And I‚Äôm super ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@khadijaarrh U really did a very good job dear</td>\n",
       "      <td>0</td>\n",
       "      <td>Jos, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@AITraderGlobal Very brilliant &amp;amp; interesti...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abuja,Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@NastyBlaq Thank you very much @NastyBlaq  eve...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@brownelixir It‚Äôs so good. Bar Insecure, I can...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@silasadedoyin @Abiodun0x 1. It‚Äôs very possibl...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>even after such a traumatic near death experie...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@JefferyAnkamah @phil_Adesh @AkosuaAmpofowah G...</td>\n",
       "      <td>2</td>\n",
       "      <td>Federal Capital Territory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@con2etal üòÇ the man dey give me joy shaa..I re...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@jon_d_doe @teejay347t \"Very ideal o. Obviousl...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@xygort Exactly. I really love hometown. It ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Really, it pays very well to govern well, I ju...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>My G, you‚Äôre always a good commander @zaMusbey...</td>\n",
       "      <td>60</td>\n",
       "      <td>Kaduna, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@Naija_PR @cool_yaks This is very good coming ...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abuja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@dashofPinknLove @Tofunmi_eb @mytakesbbn6 And ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@Feyisparkles @stgorzhye Very good so the Exod...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@Sir_Fin @TousenCollins That Ihenacho bit is a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@DarrenBent The VAR is very inconsistent and i...</td>\n",
       "      <td>0</td>\n",
       "      <td>Jos, Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@WoleOluyemiCo I am really sorry bit this pray...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abuja, Nigeria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweets  likes  \\\n",
       "0   @AITraderGlobal Very brilliant &amp; interesti...      0   \n",
       "1   Men really love the mean girls. And I‚Äôm super ...      0   \n",
       "2      @khadijaarrh U really did a very good job dear      0   \n",
       "3   @AITraderGlobal Very brilliant &amp; interesti...      3   \n",
       "4   @NastyBlaq Thank you very much @NastyBlaq  eve...      0   \n",
       "5   @brownelixir It‚Äôs so good. Bar Insecure, I can...      0   \n",
       "6   @silasadedoyin @Abiodun0x 1. It‚Äôs very possibl...      0   \n",
       "7   even after such a traumatic near death experie...      0   \n",
       "8   @JefferyAnkamah @phil_Adesh @AkosuaAmpofowah G...      2   \n",
       "9   @con2etal üòÇ the man dey give me joy shaa..I re...      1   \n",
       "10  @jon_d_doe @teejay347t \"Very ideal o. Obviousl...      1   \n",
       "11  @xygort Exactly. I really love hometown. It ma...      0   \n",
       "12  Really, it pays very well to govern well, I ju...      0   \n",
       "13  My G, you‚Äôre always a good commander @zaMusbey...     60   \n",
       "14  @Naija_PR @cool_yaks This is very good coming ...     14   \n",
       "15  @dashofPinknLove @Tofunmi_eb @mytakesbbn6 And ...      1   \n",
       "16  @Feyisparkles @stgorzhye Very good so the Exod...     12   \n",
       "17  @Sir_Fin @TousenCollins That Ihenacho bit is a...      0   \n",
       "18  @DarrenBent The VAR is very inconsistent and i...      0   \n",
       "19  @WoleOluyemiCo I am really sorry bit this pray...      0   \n",
       "\n",
       "                     location  \n",
       "0                       Abuja  \n",
       "1              Abuja, Nigeria  \n",
       "2                Jos, Nigeria  \n",
       "3               Abuja,Nigeria  \n",
       "4              Abuja, Nigeria  \n",
       "5                              \n",
       "6              Abuja, Nigeria  \n",
       "7              Abuja, Nigeria  \n",
       "8   Federal Capital Territory  \n",
       "9              Abuja, Nigeria  \n",
       "10             Abuja, Nigeria  \n",
       "11             Abuja, Nigeria  \n",
       "12             Abuja, Nigeria  \n",
       "13            Kaduna, Nigeria  \n",
       "14                      Abuja  \n",
       "15             Abuja, Nigeria  \n",
       "16             Abuja, Nigeria  \n",
       "17             Abuja, Nigeria  \n",
       "18               Jos, Nigeria  \n",
       "19             Abuja, Nigeria  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_short.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short['label'] = 'not vulgar'\n",
    "df['label'] = 'vulgar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat([df, df_short], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seYVln8U2qSM"
   },
   "source": [
    "### Making a copy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "GPLIvZ762o9J",
    "outputId": "71999994-de3f-4689-eb5a-22571ef5505c"
   },
   "outputs": [],
   "source": [
    "df_1 = df_total.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH7lEQ_mouR8"
   },
   "source": [
    "# **CLEANING THE TEXT DATA**\n",
    "### The Text data needs to be cleaned before being fed into the model for training and testing.\n",
    "    - Normalizing \n",
    "    - Remove Unicode Characters\n",
    "    - Remove Stopwords\n",
    "    - Perform Stemming\n",
    "    - Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoO5jYXboXC9"
   },
   "source": [
    "**Normalizing the texts..**\n",
    "  - changing all to lowercases also known as _case normalization_\n",
    "    - creating a function to used for changing the case to a lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "R7CFStpCoeXA"
   },
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "  text = text.lower()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBXtyATpssxe"
   },
   "source": [
    "**Removing Unicode Characters**\n",
    "  - Creating a function to remove unicode characters\n",
    "    - This function uses regular expression library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hS38jjfHsrDi"
   },
   "outputs": [],
   "source": [
    "def unicode_removal(text):\n",
    "  text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmOHc9EGtgpy"
   },
   "source": [
    "# **STOP WORDS**\n",
    "#### Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document. Generally, the most common words used in a text are ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúin‚Äù, ‚Äúfor‚Äù, ‚Äúwhere‚Äù, ‚Äúwhen‚Äù, ‚Äúto‚Äù, ‚Äúat‚Äù etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7anwURsAtFp3"
   },
   "source": [
    "**Removing Stopwords**\n",
    "  - Creating a function to remove the stop words\n",
    "    - This function uses the Natural Language ToolKit library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dDio_bL3tEOG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\oyeni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# downloading the stopwords needed\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stopwords_removal(text):\n",
    "  stop = stopwords.words('english')\n",
    "  text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQVXP4r3vNYI"
   },
   "source": [
    "# Stemming\n",
    "\n",
    "#### Stemming nvolves grouping of words by their root stem. This makes it clear or helps recognize that ‚Äòjumping‚Äô ‚Äòjumps‚Äô and ‚Äòjumped‚Äô are all rooted to the same verb (jump) and thus are referring to similar problems.\n",
    "  - Creating a function to stem the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "le-mg_VcuXWU"
   },
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "  stemmer = PorterStemmer()\n",
    "  text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4TT60BEvvu5"
   },
   "source": [
    "# Lemmatization\n",
    "\n",
    "#### Lemmatization groups words based on root definition, and helps to differentiate between present, past, and indefinite.\n",
    "\n",
    "#### In order words, ‚Äòjumps‚Äô and ‚Äòjump‚Äô are grouped into the present ‚Äòjump‚Äô, as different from all uses of ‚Äòjumped‚Äô which are grouped together as past tense, and all instances of ‚Äòjumping‚Äô which are grouped together as the indefinite (meaning continuing/continuous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2l-LIhFwDpB",
    "outputId": "504f163d-d386-4617-df3e-514e25c28271"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\oyeni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "def lemmatize(text):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy9zYvVa2d6L"
   },
   "source": [
    "### Selecting the useful feature we need to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "nCuNsvVu2kO9"
   },
   "outputs": [],
   "source": [
    "x = df_1['tweets']\n",
    "y = df_1['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aM1K7fz1i96"
   },
   "source": [
    "### Passing the text one by one through each of the functions created above using the lambda function method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "5XYqiBGBxPqS"
   },
   "outputs": [],
   "source": [
    "x = x.apply(lambda x: lowercase(x))\n",
    "x = x.apply(lambda x: unicode_removal(x))\n",
    "x = x.apply(lambda x: stopwords_removal(x))\n",
    "x = x.apply(lambda x: stemming(x))\n",
    "x = x.apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.apply(lambda y:1 if y == 'vulgar' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ni1Afut2agL"
   },
   "source": [
    "### Splitting the dataset into training and testing sets using the train test split..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "yi_bVMA2xa2c"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =  train_test_split(x, y, test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Count vectorizer to vectorize the preprocessed texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer =  CountVectorizer()\n",
    "x_train_vect = vectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(x_train_vect.toarray(),y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vect = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_vect.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the accuracy score\n",
    "###    - f1 score\n",
    "###    - precision\n",
    "###    - recall\n",
    "###    - and the _confusion matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.8382196162046909\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification reports:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.46      0.37       385\n",
      "           1       0.93      0.88      0.91      3367\n",
      "\n",
      "    accuracy                           0.84      3752\n",
      "   macro avg       0.62      0.67      0.64      3752\n",
      "weighted avg       0.87      0.84      0.85      3752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('classification reports:\\n',classification_report(y_test,model.predict(x_test_vect.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model has 84% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving our decision tree model using pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('tweet.pkl','wb') as myfile:\n",
    "    pickle.dump(model,myfile)\n",
    "\n",
    "with open('tweet.pkl','rb') as myfile:\n",
    "    model = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"sapa is not nice o na why you dey do like mad man\"]\n",
    "text_vect = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(text_vect.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
